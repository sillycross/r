<dec f='WebKit/Source/WebCore/platform/audio/ReverbConvolverStage.h' l='50' type='void WebCore::ReverbConvolverStage::ReverbConvolverStage(const float * impulseResponse, size_t responseLength, size_t reverbTotalLatency, size_t stageOffset, size_t stageLength, size_t fftSize, size_t renderPhase, size_t renderSliceSize, WebCore::ReverbAccumulationBuffer * , float scale, bool directMode = false)'/>
<def f='WebKit/Source/WebCore/platform/audio/ReverbConvolverStage.cpp' l='45' ll='101' type='void WebCore::ReverbConvolverStage::ReverbConvolverStage(const float * impulseResponse, size_t , size_t reverbTotalLatency, size_t stageOffset, size_t stageLength, size_t fftSize, size_t renderPhase, size_t renderSliceSize, WebCore::ReverbAccumulationBuffer * accumulationBuffer, float scale, bool directMode = false)'/>
<doc f='WebKit/Source/WebCore/platform/audio/ReverbConvolverStage.h' l='48'>// renderPhase is useful to know so that we can manipulate the pre versus post delay so that stages will perform
    // their heavy work (FFT processing) on different slices to balance the load in a real-time thread.</doc>
